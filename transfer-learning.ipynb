{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6384688-2e2b-41ce-b045-617385289a73",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "There are two primary types of transfer learning from a pre-trained CNN model:\n",
    "\n",
    "* Feature Extraction\n",
    "* Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142c8c9-8def-4f70-ba1e-009cebd6625c",
   "metadata": {},
   "source": [
    "### Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930327ea-b46d-439f-9d40-7ef43907aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 512, 512]         9,408\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 512, 512]         128\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 512, 512]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 256, 256]         --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 256, 256]        --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 256, 256]        75,008\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 256, 256]        70,400\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 256, 256]        70,400\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 128, 128]        --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 128, 128]        379,392\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 128, 128]        280,064\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 128, 128]        280,064\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 128, 128]        280,064\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 64, 64]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 64, 64]         1,512,448\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 64, 64]         1,117,184\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 32, 32]         --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 32, 32]         6,039,552\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 32, 32]         4,462,592\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 32, 32]         4,462,592\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear (fc): 1-10                                [1, 1000]                 2,049,000\n",
       "====================================================================================================\n",
       "Total params: 25,557,032\n",
       "Trainable params: 25,557,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 85.41\n",
       "====================================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 3716.16\n",
       "Params size (MB): 102.23\n",
       "Estimated Total Size (MB): 3830.97\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# load ResNet50 model as feature extractor\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "summary(model, (1, 3, 1024, 1024), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7714c1b-b9cb-4db5-a6d0-8306c6ead616",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Transfer learning means **retraining the final layer** of a deep network. Not only is this useful for solving problems with **limited training examples**, but also when you don't have adequate **computing resources** to train a network from scratch. \n",
    "\n",
    "However, if you have sufficient data, adapting weights via transfer learning is not preferable because the features that were extracted from the original training process are unlikely to be ideal for another application.\n",
    "\n",
    "Feature extraction in the context of a **CNN** is not necessarily an explicit process, rather a sort of high-level product of the training process. Feature extraction refers to the portion of the training process by which a CNN learns to map input space to a latent space that can subsequently be used for classification via the final layer. \n",
    "\n",
    "In other words, the hidden layers learn discriminatory features in the form of weight-adjusted convolutional filters. Thus the term \"feature extraction\" generally refers to the portion of the training process that occurs before the final layer. So it is not part of transfer learning in which only the last layer is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c71cf-e88c-4cfd-848f-2760f237487a",
   "metadata": {},
   "source": [
    "### Create Model for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972d24d3-b9bc-4712-8b0c-c3f7eb57f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 512, 512]         (9,408)\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 512, 512]         (128)\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 512, 512]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 256, 256]         --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 256, 256]        --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 256, 256]        (75,008)\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 256, 256]        (70,400)\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 256, 256]        (70,400)\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 128, 128]        --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 128, 128]        (379,392)\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 128, 128]        (280,064)\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 128, 128]        (280,064)\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 128, 128]        (280,064)\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 64, 64]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 64, 64]         (1,512,448)\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 64, 64]         (1,117,184)\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 32, 32]         --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 32, 32]         (6,039,552)\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 32, 32]         (4,462,592)\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 32, 32]         (4,462,592)\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear (fc): 1-10                                [1, 5]                    10,245\n",
       "====================================================================================================\n",
       "Total params: 23,518,277\n",
       "Trainable params: 10,245\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (G): 85.41\n",
       "====================================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 3716.15\n",
       "Params size (MB): 94.07\n",
       "Estimated Total Size (MB): 3822.81\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# load ResNet50 model as feature extractor\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# freeze parameters to non-trainable (by default they are trainable)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# append a new classification top to our feature extractor and pop it on to the current device\n",
    "num_features = model.fc.in_features\n",
    "num_classes = 5\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "summary(model, (1, 3, 1024, 1024), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca1f5f-63ce-471a-9238-da3ff3427716",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "On **transfer learning** we remove the FC layer head from the pre-trained network, but this time we construct a new, freshly initialized FC layer head and place it on top of the original body of the network. \n",
    "\n",
    "The weights in the body of the CNN are frozen, and then we train the new layer head (typically with a very small learning rate). We may then choose to unfreeze the body of the network and train the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc883c4-d918-4480-ba38-8c2f9bc9862b",
   "metadata": {},
   "source": [
    "### Create Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c23c54-28b8-4544-a16e-f157e9e6cae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 512, 512]         9,408\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 512, 512]         128\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 512, 512]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 256, 256]         --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 256, 256]        --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 256, 256]        75,008\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 256, 256]        70,400\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 256, 256]        70,400\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 128, 128]        --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 128, 128]        379,392\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 128, 128]        280,064\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 128, 128]        280,064\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 128, 128]        280,064\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 64, 64]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 64, 64]         1,512,448\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 64, 64]         1,117,184\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 64, 64]         1,117,184\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 32, 32]         --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 32, 32]         6,039,552\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 32, 32]         4,462,592\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 32, 32]         4,462,592\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Sequential (fc): 1-10                            [1, 5]                    --\n",
       "│    └─Linear (0): 2-17                            [1, 512]                  1,049,088\n",
       "│    └─ReLU (1): 2-18                              [1, 512]                  --\n",
       "│    └─Dropout (2): 2-19                           [1, 512]                  --\n",
       "│    └─Linear (3): 2-20                            [1, 256]                  131,328\n",
       "│    └─ReLU (4): 2-21                              [1, 256]                  --\n",
       "│    └─Dropout (5): 2-22                           [1, 256]                  --\n",
       "│    └─Linear (6): 2-23                            [1, 5]                    1,285\n",
       "====================================================================================================\n",
       "Total params: 24,689,733\n",
       "Trainable params: 17,688,709\n",
       "Non-trainable params: 7,001,024\n",
       "Total mult-adds (G): 85.41\n",
       "====================================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 3716.16\n",
       "Params size (MB): 98.76\n",
       "Estimated Total Size (MB): 3827.50\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# load ResNet50 model for fine tuning\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "# loop over the modules of the model and set the parameters of batch normalization modules as not trainable\n",
    "for module, param in zip(model.modules(), model.parameters()):\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# define the network head and attach it to the model\n",
    "num_classes = 5\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 1024, 1024), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04dcbd2-7fbc-41b2-88c6-92810bdecace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
