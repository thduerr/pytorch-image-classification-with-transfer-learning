{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6384688-2e2b-41ce-b045-617385289a73",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "source: https://cs231n.github.io/transfer-learning/\n",
    "\n",
    "Typically CNNs are not trained from scratch (with random initialization) because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a CNN on a very large dataset and then use the CNN either as an initialization or a fixed feature extractor for the task of interest.\n",
    "\n",
    "The pre-traines model in the example is **ResNet-50**, a convolutional neural network that is 50 layers deep. THe pretrained version of the network is trained on more than a million images from the **ImageNet database** and can classify images into **1000 object categories**. As a result, the network has learned rich feature representations for a wide range of images. The network has an image **input size of 224x224**.\n",
    "\n",
    "There are three primary types of transfer learning from a pre-trained CNN model:\n",
    "\n",
    "1. Pretrained Model\n",
    "1. Feature Extraction\n",
    "1. Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142c8c9-8def-4f70-ba1e-009cebd6625c",
   "metadata": {},
   "source": [
    "### Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930327ea-b46d-439f-9d40-7ef43907aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 112, 112]         128\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 112, 112]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 56, 56]          75,008\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 56, 56]          70,400\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 56, 56]          70,400\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 28, 28]          379,392\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 28, 28]          280,064\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 28, 28]          280,064\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 28, 28]          280,064\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 14, 14]         1,512,448\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 14, 14]         1,117,184\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 7, 7]           6,039,552\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 7, 7]           4,462,592\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 7, 7]           4,462,592\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear (fc): 1-10                                [1, 1000]                 2,049,000\n",
       "====================================================================================================\n",
       "Total params: 25,557,032\n",
       "Trainable params: 25,557,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.09\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.83\n",
       "Params size (MB): 102.23\n",
       "Estimated Total Size (MB): 280.66\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# load ResNet50 model as feature extractor\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "summary(model, (1, 3, 224, 224), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7714c1b-b9cb-4db5-a6d0-8306c6ead616",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Here we remove the last fully-connected layer `(fc)`, then treat the rest of the CNN as a **fixed feature extractor** for the new dataset. For ResNet-50, this computes a 2048-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features **CNN codes**. Once you extract the 2048-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.\n",
    "\n",
    "Transfer learning means **retraining the final layer** of a deep network. Not only is this useful for solving problems with **limited training examples**, but also when you don't have adequate **computing resources** to train a network from scratch. \n",
    "\n",
    "However, if you have sufficient data, adapting weights via transfer learning is not preferable because the features that were extracted from the original training process are unlikely to be ideal for another application.\n",
    "\n",
    "Feature extraction in the context of a **CNN** is not necessarily an explicit process, rather a sort of high-level product of the training process. Feature extraction refers to the portion of the training process by which a CNN learns to map input space to a latent space that can subsequently be used for classification via the final layer. \n",
    "\n",
    "In other words, the hidden layers learn discriminatory features in the form of weight-adjusted convolutional filters. Thus the term \"feature extraction\" generally refers to the portion of the training process that occurs before the final layer. So it is not part of transfer learning in which only the last layer is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c71cf-e88c-4cfd-848f-2760f237487a",
   "metadata": {},
   "source": [
    "### Create ResNET Model for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972d24d3-b9bc-4712-8b0c-c3f7eb57f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 112, 112]         (9,408)\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 112, 112]         (128)\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 112, 112]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 56, 56]          (75,008)\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 56, 56]          (70,400)\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 56, 56]          (70,400)\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 28, 28]          (379,392)\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 28, 28]          (280,064)\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 28, 28]          (280,064)\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 28, 28]          (280,064)\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 14, 14]         (1,512,448)\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 14, 14]         (1,117,184)\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 14, 14]         (1,117,184)\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 7, 7]           (6,039,552)\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 7, 7]           (4,462,592)\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 7, 7]           (4,462,592)\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear (fc): 1-10                                [1, 5]                    10,245\n",
       "====================================================================================================\n",
       "Total params: 23,518,277\n",
       "Trainable params: 10,245\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (G): 4.09\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.82\n",
       "Params size (MB): 94.07\n",
       "Estimated Total Size (MB): 272.50\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# load ResNet50 model as feature extractor\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# freeze parameters to non-trainable (by default they are trainable)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# append a new classification top to our feature extractor and pop it on to the current device\n",
    "num_features = model.fc.in_features\n",
    "num_classes = 5\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "summary(model, (1, 3, 224, 224), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca1f5f-63ce-471a-9238-da3ff3427716",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "Here we not only replace and retrain the classifier on top of the CNN on the new dataset, but to also **fine-tune the weights** of the pretrained network by continuing the backpropagation. \n",
    "\n",
    "It is possible to fine-tune all the layers of the CNN, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the **earlier features** of a CNN contain more **generic features** (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but **later layers** of the CNN becomes progressively more specific to the **details of** the classes contained in the **original dataset**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc883c4-d918-4480-ba38-8c2f9bc9862b",
   "metadata": {},
   "source": [
    "### Create a Fine-Tuned ResNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c23c54-28b8-4544-a16e-f157e9e6cae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 112, 112]         128\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 112, 112]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 56, 56]          75,008\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 56, 56]          70,400\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 56, 56]          70,400\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 28, 28]          379,392\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 28, 28]          280,064\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 28, 28]          280,064\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 28, 28]          280,064\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 14, 14]         1,512,448\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 14, 14]         1,117,184\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 14, 14]         1,117,184\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 7, 7]           6,039,552\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 7, 7]           4,462,592\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 7, 7]           4,462,592\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Sequential (fc): 1-10                            [1, 5]                    --\n",
       "│    └─Linear (0): 2-17                            [1, 512]                  1,049,088\n",
       "│    └─ReLU (1): 2-18                              [1, 512]                  --\n",
       "│    └─Dropout (2): 2-19                           [1, 512]                  --\n",
       "│    └─Linear (3): 2-20                            [1, 256]                  131,328\n",
       "│    └─ReLU (4): 2-21                              [1, 256]                  --\n",
       "│    └─Dropout (5): 2-22                           [1, 256]                  --\n",
       "│    └─Linear (6): 2-23                            [1, 5]                    1,285\n",
       "====================================================================================================\n",
       "Total params: 24,689,733\n",
       "Trainable params: 17,688,709\n",
       "Non-trainable params: 7,001,024\n",
       "Total mult-adds (G): 4.09\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.83\n",
       "Params size (MB): 98.76\n",
       "Estimated Total Size (MB): 277.19\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "# load ResNet50 model for fine tuning\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "# loop over the modules of the model and set the parameters of batch normalization modules as not trainable\n",
    "for module, param in zip(model.modules(), model.parameters()):\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# define the network head and attach it to the model\n",
    "num_classes = 5\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "summary(model, (1, 3, 224, 224), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c070f1-d7d7-42c3-adbe-805a0b2babb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
