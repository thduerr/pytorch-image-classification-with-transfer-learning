{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6384688-2e2b-41ce-b045-617385289a73",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "There are two primary types of transfer learning:\n",
    "\n",
    "* Feature Extraction\n",
    "* Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7714c1b-b9cb-4db5-a6d0-8306c6ead616",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Transfer learning means **retraining the final layer** of a deep network. Not only is this useful for solving problems with **limited training examples**, but also when you don't have adequate **computing resources** to train a network from scratch. \n",
    "\n",
    "However, if you have sufficient data, adapting weights via transfer learning is not preferable because the features that were extracted from the original training process are unlikely to be ideal for another application.\n",
    "\n",
    "Feature extraction in the context of a **CNN** is not necessarily an explicit process, rather a sort of high-level product of the training process. Feature extraction refers to the portion of the training process by which a CNN learns to map input space to a latent space that can subsequently be used for classification via the final layer. \n",
    "\n",
    "In other words, the hidden layers learn discriminatory features in the form of weight-adjusted convolutional filters. Thus the term \"feature extraction\" generally refers to the portion of the training process that occurs before the final layer. So it is not part of transfer learning in which only the last layer is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf867a6b-5166-46b2-aba9-46f05e206cf8",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c359f78b-c3a5-4d3b-801f-a87becae7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch import config\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchinfo import summary\n",
    "import os\n",
    "\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "# torch.multiprocessing.freeze_support()\n",
    "\n",
    "train_tansforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(config.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config.MEAN, std=config.STD)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=config.TRAIN, transform=train_tansforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.FEATURE_EXTRACTION_BATCH_SIZE, shuffle=True, num_workers=os.cpu_count(), pin_memory=True if config.DEVICE == \"cuda\" else False)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root=config.VAL, transform=val_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.FEATURE_EXTRACTION_BATCH_SIZE, shuffle=False, num_workers=os.cpu_count(), pin_memory=True if config.DEVICE == \"cuda\" else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c71cf-e88c-4cfd-848f-2760f237487a",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972d24d3-b9bc-4712-8b0c-c3f7eb57f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                  Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             --                        --\n",
       "├─Conv2d (conv1): 1-1                              [1, 64, 512, 512]         (9,408)\n",
       "├─BatchNorm2d (bn1): 1-2                           [1, 64, 512, 512]         (128)\n",
       "├─ReLU (relu): 1-3                                 [1, 64, 512, 512]         --\n",
       "├─MaxPool2d (maxpool): 1-4                         [1, 64, 256, 256]         --\n",
       "├─Sequential (layer1): 1-5                         [1, 256, 256, 256]        --\n",
       "│    └─Bottleneck (0): 2-1                         [1, 256, 256, 256]        (75,008)\n",
       "│    └─Bottleneck (1): 2-2                         [1, 256, 256, 256]        (70,400)\n",
       "│    └─Bottleneck (2): 2-3                         [1, 256, 256, 256]        (70,400)\n",
       "├─Sequential (layer2): 1-6                         [1, 512, 128, 128]        --\n",
       "│    └─Bottleneck (0): 2-4                         [1, 512, 128, 128]        (379,392)\n",
       "│    └─Bottleneck (1): 2-5                         [1, 512, 128, 128]        (280,064)\n",
       "│    └─Bottleneck (2): 2-6                         [1, 512, 128, 128]        (280,064)\n",
       "│    └─Bottleneck (3): 2-7                         [1, 512, 128, 128]        (280,064)\n",
       "├─Sequential (layer3): 1-7                         [1, 1024, 64, 64]         --\n",
       "│    └─Bottleneck (0): 2-8                         [1, 1024, 64, 64]         (1,512,448)\n",
       "│    └─Bottleneck (1): 2-9                         [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (2): 2-10                        [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (3): 2-11                        [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (4): 2-12                        [1, 1024, 64, 64]         (1,117,184)\n",
       "│    └─Bottleneck (5): 2-13                        [1, 1024, 64, 64]         (1,117,184)\n",
       "├─Sequential (layer4): 1-8                         [1, 2048, 32, 32]         --\n",
       "│    └─Bottleneck (0): 2-14                        [1, 2048, 32, 32]         (6,039,552)\n",
       "│    └─Bottleneck (1): 2-15                        [1, 2048, 32, 32]         (4,462,592)\n",
       "│    └─Bottleneck (2): 2-16                        [1, 2048, 32, 32]         (4,462,592)\n",
       "├─AdaptiveAvgPool2d (avgpool): 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear (fc): 1-10                                [1, 5]                    10,245\n",
       "====================================================================================================\n",
       "Total params: 23,518,277\n",
       "Trainable params: 10,245\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (G): 85.41\n",
       "====================================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 3716.15\n",
       "Params size (MB): 94.07\n",
       "Estimated Total Size (MB): 3822.81\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ResNet50 model as feature extractor\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# freeze parameters to non-trainable (by default they are trainable)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# append a new classification top to our feature extractor and pop it on to the current device\n",
    "outFeatures = model.fc.in_features\n",
    "model.fc = nn.Linear(outFeatures, len(train_dataset.classes))\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "# initialize loss function and optimizer (notice that we are only providing the parameters of the classification top to our optimizer)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=config.LR)\n",
    "#optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "summary(model, (1, 3, 1024, 1024), row_settings=('depth', 'var_names'), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3309e75-d77b-4187-84e5-9a08f730020a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4de2e-3479-468e-9997-a9da31861ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█████▊                                                                                                                                                                         | 1/30 [02:04<1:00:01, 124.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/30\n",
      "Train loss: 1.700284, Train accuracy: 0.3406\n",
      "Val loss: 2.758565, Val accuracy: 0.5036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████████▊                                                                                                                                                                     | 2/30 [04:07<57:42, 123.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/30\n",
      "Train loss: 1.339692, Train accuracy: 0.6149\n",
      "Val loss: 1.874355, Val accuracy: 0.6752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████▋                                                                                                                                                               | 3/30 [06:10<55:32, 123.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/30\n",
      "Train loss: 1.106096, Train accuracy: 0.7079\n",
      "Val loss: 1.728146, Val accuracy: 0.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████████████▌                                                                                                                                                         | 4/30 [08:13<53:28, 123.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/30\n",
      "Train loss: 0.967500, Train accuracy: 0.7597\n",
      "Val loss: 1.390400, Val accuracy: 0.7701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████████████████▌                                                                                                                                                   | 5/30 [10:18<51:31, 123.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/30\n",
      "Train loss: 0.863759, Train accuracy: 0.7605\n",
      "Val loss: 1.329366, Val accuracy: 0.7628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████████▍                                                                                                                                             | 6/30 [12:21<49:27, 123.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 6/30\n",
      "Train loss: 0.788425, Train accuracy: 0.7836\n",
      "Val loss: 1.122735, Val accuracy: 0.8029\n"
     ]
    }
   ],
   "source": [
    "train_steps = len(train_dataset) // config.FEATURE_EXTRACTION_BATCH_SIZE\n",
    "val_steps = len(val_dataset) // config.FEATURE_EXTRACTION_BATCH_SIZE\n",
    "\n",
    "log = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in tqdm(range(config.EPOCHS)):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "\n",
    "    # loop over the training set\n",
    "    for (batch_idx, (X, y)) in enumerate(train_loader):\n",
    "\n",
    "        (X, y) = (X.to(config.DEVICE), y.to(config.DEVICE))\n",
    "        pred = model(X)            # perform a forward pass\n",
    "        loss = loss_fn(pred, y)    # calculate the training loss\n",
    "        loss.backward()            # calculate the gradients\n",
    "\n",
    "        # check if we are updating the model parameters and if so update them, and zero out the previously accumulated gradients\n",
    "        if (batch_idx + 2) % 2 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss\n",
    "        train_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (X, y) in val_loader:\n",
    "            (X, y) = (X.to(config.DEVICE), y.to(config.DEVICE))\n",
    "            pred = model(X)\n",
    "            total_val_loss += loss_fn(pred, y)\n",
    "            val_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avg_train_loss = total_train_loss / train_steps\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "\n",
    "    # calculate the training and validation accuracy\n",
    "    train_correct = train_correct / len(train_dataset)\n",
    "    val_correct = val_correct / len(val_dataset)\n",
    "\n",
    "    # update our training history\n",
    "    log['train_loss'].append(avg_train_loss.cpu().detach().numpy())\n",
    "    log['train_acc'].append(train_correct)\n",
    "    log['val_loss'].append(avg_val_loss.cpu().detach().numpy())\n",
    "    log['val_acc'].append(val_correct)\n",
    "\n",
    "    # print the model training and validation information\n",
    "    print(f'EPOCH: {epoch + 1}/{config.EPOCHS}')\n",
    "    print(f'Train Loss: {avg_train_loss:.6f}, Train Accuracy: {train_correct:.4f}')\n",
    "    print(f'Validation Loss: {avg_val_loss:.6f}, Validation Accuracy: {val_correct:.4f}')\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "end_time = time.time()\n",
    "print(f'Total time to train the model: {(end_time - start_time):.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513768f-5eef-423b-a70b-77610e3be1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(log['train_loss'], label='train loss')\n",
    "plt.plot(log['val_loss'], label='val loss')\n",
    "plt.plot(log['train_acc'], label='train acc')\n",
    "plt.plot(log['val_acc'], label='val acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d249b-6e7c-4cd6-8b9e-10395d5ce2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1240857-cbd1-478b-ac5c-c55e671f188d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ca1f5f-63ce-471a-9238-da3ff3427716",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "Transfer learning via **fine-tuning**: When applying fine-tuning, we again remove the FC layer head from the pre-trained network, but this time we construct a brand new, freshly initialized FC layer head and place it on top of the original body of the network. The weights in the body of the CNN are frozen, and then we train the new layer head (typically with a very small learning rate). We may then choose to unfreeze the body of the network and train the entire network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c23c54-28b8-4544-a16e-f157e9e6cae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
